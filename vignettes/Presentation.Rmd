---
title: "LDA und SVM"
author: "Daniel Fridljand, Benedikt Lauer, Henning Stein, Niklas Wünstel"
date: "February 22, 2019"
output:
  ioslides_presentation: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(gridExtra)
library(quadprog)
library(R6)
library(MASS)
library(NlcOptim)
library(shiny)
library(NlcOptim)
library(shiny)
library(rlang)
library(knitr)
source("R/Basis_expansion.R")
source("R/Test.R")
source("R/oop.R")
source("R/Estimators.R")
source("R/Classifier_funs.R")
source("R/plot_functions.R")
source("R/shinyplot.R")
source("R/svm.R")
source("R/Calc_error.R")
set.seed(2)
height_B <- rnorm(115, mean = 30, sd = 4)
decibel_B <- rnorm(115, mean = 70, sd = 10)
classes_B <- rep("Barock", 115)

height_R <- rnorm(78, mean = 17, sd = 6)
decibel_R <- rnorm(78, mean = 102, sd = 7)
classes_R <- rep("Renaissance", 78)

height_C <- rnorm(96, mean = 42, sd = 7)
decibel_C <- rnorm(96, mean = 64, sd = 8)
classes_C <- rep("Classik", 96)

height <- c(height_B, height_R, height_C)
decibel <- c(decibel_B, decibel_R, decibel_C)
class <- c(classes_B, classes_R, classes_C)

Rockets_data <- data.frame(decibel = decibel, height = height, class = class)
Rockets_data[] <- Rockets_data[sample.int(nrow(Rockets_data)), ]
validation <- Rockets_data[1:20,1:2]
goals <- Rockets_data[1:20,'class']
Rockets <- Rockets_data
set.seed(2)
```

## Das Problem

- Ausgangspunkt
    - Daten in Klassen aufteilbar 

- Ziel
    - Klassifizierung neuer Daten <!-- automatisch -->
    
- Weg 
    - Trainingsdaten sammeln und Klassifizierungsfunktionen berechnen

## Trainingsdaten

```{r structure data-frame, echo = TRUE}
head(Rockets, 10)
```

## Datenplot

```{r plot Example, echo = TRUE, results='hide'}

Rocket_set <- make_set(Rockets, by = "class", title = "Rockets")
make_2D_plot(Rocket_set)
```

## Linear Disriminant Analysis (LDA)

- Annahmen
    - normalverteilte Daten
    - gleiche Varianz in jeder Klasse

- Klassengrenzen linear

## LDA Klassifizierung

```{r LDA plot,results = 'hide',fig.keep='last'}
func <- LDA(Rocket_set) ##Creating the classifier function with LDA. 'name' is the slot in which the function is in
testplot1 <- make_2D_plot(Rocket_set, ##Creating a ggplot
                          func[['name']], ##Function by which should be classified
                          ppu = 1,
                          project = FALSE) ##Resolution of the background

testplot1 ##Actual plotting
```

## Quadratic Discriminant Analysis (QDA)

- Annahmen
    - normalverteilte Daten
    - unterschiedliche Varianzen für unterschiedliche Klassen
    
- Klassengrenzen quadratisch

## QDA Klassifizierung

```{r QDA plot,results = 'hide',fig.keep='last'}
func2 <- QDA(Rocket_set) ##Creating the classifier function with QDA. 'name' is the slot in which the function is in
testplot2 <-make_2D_plot(Rocket_set, ##Creating a ggplot object where the classification image is saved in
                          func2[['name']], ##Function by which should be classified
                          ppu = 1,
                          project = FALSE) ##Resolution of the background
testplot2 ##Actual plotting
```

## Regular Discriminant Analysis (RDA)

- Annahmen
    - normalverteilte Daten
    - unterschiedliche Varianzen für unterschiedliche Klassen
    
- Kombination aus LDA und QDA <!-- alpha -->
- Kombination aus mehr- und eindimensionaler Verteilung <!-- gamma -->

## RDA Klassifizierung

```{r RDA plot, results = 'hide',fig.keep='last'}
func3 <- RDA(Rocket_set, alpha = 0.5, gamma = 0.5) ##Creating the classifier function with RDA. 'name' is the slot in which the function is in
testplot3 <- make_2D_plot(Rocket_set, ##Creating a ggplot object where the classification image is saved in
                          func3[['name']], ##Function by which should be classified
                          ppu = 1,
                          project = FALSE) ##Resolution of the background
testplot3 ##Actual plotting
```

## Penalized Discriminant Analysis (PDA)

- Annahmen
    - normalverteilte Daten
    - unterschiedliche Varianzen für unterschiedliche Klassen
    
- Basis Expansion
- Penalizer Matrix <!-- Matrix, die auf die Kovarianz addiert wird um bestimmte Koordinaten weniger oder mehr zu berücksichtigen (kleines
                        Vielfaches der Einheitsmatrix oder Cross - validation) -->

## PDA Klassifizierung

```{r PDA plot, results = 'hide',fig.keep='last'}
func4 <- PDA(Rocket_set, base = "cube") ##Creating the classifier function with PDA. 'name' is the slot in which the function is in
testplot4 <- make_2D_plot(Rocket_set, ##Creating a ggplot object where the classification image is saved in
                          func4[['name']], ##Function by which should be classified
                          ppu = 1,
                          project = FALSE) ##Resolution of the background
testplot4 ##Actual plotting
```

## Support Vector Machines (SVM)

- Annahmen
    - keine
    
- Nur Randpunkte für Klassifikation relevant (Support Vectors) <!-- C = Angabe über Genauigkeit der Klassifikation (je größer desto genauer) -->
- Kernels <!-- klügere Art der Basisexpansion, betrachte Daten in Höherdimensionalen Raum, wo Trennungen einfacher sind -->  

## SVM Klassifizierung

```{r SVM plot, results = 'hide', fig.keep='last'}
func5 <- SVM(Rocket_set, C = 1, kernel = "radial", g = 1) ##Creating the classifier function with SVM. 'name' is the slot in which the function is in
testplot5 <- make_2D_plot(Rocket_set, ##Creating a ggplot object where the classification image is saved in
                          func5[['name']], ##Function by which should be classified
                          ppu = 1,
                          project = FALSE) ##Resolution of the background
testplot5 ##Actual plotting
```

## Vergleich

```{r table, echo=FALSE, message=FALSE, warnings=FALSE, results='hide'}
#library(kableExtra)
# Vergleich <- data.frame(Methode = c("LDA", "QDA", "RDA", "PDA", "SVM"),
#                         Pro = c("schnell und einfach", "relativ einfach mit wenigen Annahmen", "bestes aus LDA und QDA", "gut für korrellierte   Daten", "keine Ahnnahmen und sehr genau"),
#                         Contra = c("nur lineare Trennung", "langsamer als LDA bei marginaler Verbesserung", "aufwendiges Parametertuning", "viel Wissen über Daten notwendig", "Overfitting und lange Berechnung"))
# column_spec(kable_styling(kable(Vergleich), bootstrap_options = "striped", position = "left"), 1:3, border_right = TRUE, border_left = TRUE)
```
![](Pro_con.png)

## Ende
<center>
Vielen Dank
</center>